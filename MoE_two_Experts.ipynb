{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate peft datasets wandb bitsandbytes -q"
      ],
      "metadata": {
        "id": "3GgzZ7NPU65j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    AutoModelForCausalLM, TrainingArguments, Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import wandb\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8WKD96TkYY4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()\n",
        "os.environ[\"WANDB_PROJECT\"] = \"Banking77_MoE\"\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"end\""
      ],
      "metadata": {
        "id": "M2pbWwXWbwgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/drive/MyDrive/Banking77_Project/data/train.csv\"\n",
        "test_path = \"/content/drive/MyDrive/Banking77_Project/data/test.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_path, names=[\"text\", \"label\"])\n",
        "test_df = pd.read_csv(test_path, names=[\"text\", \"label\"])\n",
        "\n",
        "# Convert labels â†’ int IDs\n",
        "label2id = {label: i for i, label in enumerate(sorted(train_df[\"label\"].unique()))}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "train_df[\"label\"] = train_df[\"label\"].map(label2id).astype(int)\n",
        "test_df[\"label\"] = test_df[\"label\"].map(label2id).astype(int)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
        "num_labels = len(label2id)\n",
        "\n",
        "print(dataset)\n",
        "print(\"Num labels:\", num_labels)\n"
      ],
      "metadata": {
        "id": "iUVW3GL7by5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_name = \"bert-base-uncased\"\n",
        "gpt2_model_name = \"gpt2\"\n",
        "\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "tokenizer_gpt2 = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
        "\n",
        "# Fix GPT2 padding\n",
        "if tokenizer_gpt2.pad_token is None:\n",
        "    tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token\n",
        "\n",
        "def tokenize_bert(examples):\n",
        "    return tokenizer_bert(\n",
        "        examples[\"text\"], truncation=True, padding=\"max_length\", max_length=64\n",
        "    )\n",
        "\n",
        "def tokenize_gpt2(examples):\n",
        "    return tokenizer_gpt2(\n",
        "        examples[\"text\"], truncation=True, padding=\"max_length\", max_length=64\n",
        "    )\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_dataset_bert = dataset.map(tokenize_bert, batched=True)\n",
        "tokenized_dataset_bert = tokenized_dataset_bert.map(lambda x: {\"labels\": x[\"label\"]})\n",
        "\n",
        "tokenized_dataset_gpt2 = dataset.map(tokenize_gpt2, batched=True)\n",
        "tokenized_dataset_gpt2 = tokenized_dataset_gpt2.map(lambda x: {\"labels\": x[\"label\"]})\n"
      ],
      "metadata": {
        "id": "nNUxihueb1u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
        "    }\n"
      ],
      "metadata": {
        "id": "VCvW6Q-vb4_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Banking77_Project/outputs\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"HybridMoE_Training\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "21a4Mn8_b7Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    bert_model_name, num_labels=num_labels\n",
        ")\n",
        "dora_config = LoraConfig(\n",
        "    r=8, lora_alpha=32, lora_dropout=0.1,\n",
        "    bias=\"none\", task_type=\"SEQ_CLS\"\n",
        ")\n",
        "bert_peft = get_peft_model(bert_model, dora_config)\n",
        "print(\"BERT+DoRA loaded\")\n"
      ],
      "metadata": {
        "id": "PPT7uIixb-tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_bert = Trainer(\n",
        "    model=bert_peft,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset_bert[\"train\"],\n",
        "    eval_dataset=tokenized_dataset_bert[\"test\"],\n",
        "    tokenizer=tokenizer_bert,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_bert.train()\n",
        "trainer_bert.evaluate()"
      ],
      "metadata": {
        "id": "OmWtxYLWcA_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "gpt2_model_name = \"gpt2\"\n",
        "\n",
        "# Load GPT2 for sequence classification\n",
        "gpt2_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    gpt2_model_name,\n",
        "    num_labels=num_labels,                   # 77 classes\n",
        "    pad_token_id=tokenizer_gpt2.pad_token_id\n",
        ")\n",
        "\n",
        "# QLoRA config\n",
        "qlora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"                     # Important: classification\n",
        ")\n",
        "\n",
        "# Wrap model with QLoRA\n",
        "gpt2_peft = get_peft_model(gpt2_model, qlora_config)\n",
        "print(\"GPT2+QLoRA classification model ready\")\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args_gpt2 = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Banking77_Project/outputs_gpt2\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs_gpt2\",\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"GPT2_QLoRA_Classification\"\n",
        ")\n",
        "\n",
        "trainer_gpt2 = Trainer(\n",
        "    model=gpt2_peft,\n",
        "    args=training_args_gpt2,\n",
        "    train_dataset=tokenized_dataset_gpt2[\"train\"],\n",
        "    eval_dataset=tokenized_dataset_gpt2[\"test\"],\n",
        "    tokenizer=tokenizer_gpt2,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_gpt2.train()\n",
        "trainer_gpt2.evaluate()\n"
      ],
      "metadata": {
        "id": "6SoMdN42fHRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class HybridMoE(nn.Module):\n",
        "    def __init__(self, bert_model, gpt2_model, num_labels):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model\n",
        "        self.gpt2 = gpt2_model\n",
        "        self.router = nn.Sequential(\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        # Expert logits\n",
        "        bert_logits = self.bert(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "        gpt2_logits = self.gpt2(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "\n",
        "        router_weights = self.router(input_ids.float()[:, :64])\n",
        "        logits = router_weights[:, 0].unsqueeze(-1) * bert_logits + router_weights[:, 1].unsqueeze(-1) * gpt2_logits\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "moe_model = HybridMoE(bert_peft, gpt2_peft, num_labels=num_labels)\n",
        "print(\"Hybrid MoE placeholder ready\")"
      ],
      "metadata": {
        "id": "qDDmL_fIcr1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_peft.save_pretrained(\"/content/drive/MyDrive/Banking77_Project/bert_dora\")\n",
        "gpt2_peft.save_pretrained(\"/content/drive/MyDrive/Banking77_Project/gpt2_qlora\")\n",
        "tokenizer_bert.save_pretrained(\"/content/drive/MyDrive/Banking77_Project/bert_dora\")\n",
        "tokenizer_gpt2.save_pretrained(\"/content/drive/MyDrive/Banking77_Project/gpt2_qlora\")\n"
      ],
      "metadata": {
        "id": "Wjv0YrP3ctrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3AXGpngDgVA6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}