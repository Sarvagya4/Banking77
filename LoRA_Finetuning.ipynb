{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAwZwPqQz3pc"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers datasets peft accelerate wandb evaluate \"scikit-learn<1.7\" -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wandb\n",
        "from huggingface_hub import HfFolder, notebook_login\n",
        "from google.colab import drive, userdata"
      ],
      "metadata": {
        "id": "aQzesI2Q0J-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "BhOkXY-F0evD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VG0sSA5_0f-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data_path_prefix = \"/content/drive/MyDrive/Banking77_Project/data/\"\n",
        "data_files = {\n",
        "    \"train\": os.path.join(data_path_prefix, \"train.csv\"),\n",
        "    \"validation\": os.path.join(data_path_prefix, \"validation.csv\"),\n",
        "    \"test\": os.path.join(data_path_prefix, \"test.csv\")\n",
        "}\n",
        "\n",
        "for split, path in data_files.items():\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"ERROR: The file for the '{split}' split was not found at: {path}\")\n",
        "        exit()\n",
        "    else:\n",
        "        print(f\" Found '{split}' file: {path}\")\n",
        "\n",
        "print(\"\\nLoading datasets...\")\n",
        "dataset = load_dataset('csv', data_files=data_files)\n",
        "\n",
        "\n",
        "print(\"\\nDataset loaded successfully:\")\n",
        "print(f\"Columns found: {dataset['train'].column_names}\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "LkBnJByA0i-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Always keep this as a string\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# --- IMPROVED LORA CONFIGURATION ---\n",
        "lora_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=128,\n",
        "    target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"\n",
        ")\n",
        "\n",
        "# Load the base model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "# Apply the LoRA adapter\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "0nHwmd-MTqER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Always pass a string model id, not the model object\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model_checkpoint = AutoModel.from_pretrained(model_checkpoint)\n",
        "\n"
      ],
      "metadata": {
        "id": "xIXs3_qoRQAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "print(\"\\nData tokenized and formatted for PyTorch.\")\n",
        "print(f\"Columns in the final tokenized dataset: {tokenized_datasets['train'].column_names}\")\n"
      ],
      "metadata": {
        "id": "KkfYaxJd0lNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from peft import get_peft_model, LoraConfig\n",
        "\n",
        "num_labels = len(dataset['train'].unique('label'))\n",
        "print(f\"\\nNumber of unique labels: {num_labels}\")"
      ],
      "metadata": {
        "id": "jU_pjMjO1wad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- IMPROVED LORA CONFIGURATION ---\n",
        "lora_config = LoraConfig(\n",
        "    # Increase the rank 'r' for more capacity. 64 is a strong starting point.\n",
        "    r=64,\n",
        "    # The convention is to set lora_alpha to be double the rank.\n",
        "    lora_alpha=128,\n",
        "    # Target ALL linear layers in the attention blocks for maximum adaptability.\n",
        "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],\n",
        "    # Increase dropout slightly to prevent overfitting with the larger rank.\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"\n",
        ")\n",
        "\n",
        "# Load the base model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "# Apply the LoRA adapter\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "_07Kd6360mbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# Load metrics\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1}"
      ],
      "metadata": {
        "id": "7jzFE9eq0ppr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"Banking77-Intent-Classification\", name=\"Day3-LoRA-Fine-tuning-Final\")"
      ],
      "metadata": {
        "id": "jRCzgMSg0sjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"lora-distilbert-banking77-improved\",\n",
        "    # Use a higher learning rate, which is often better for LoRA.\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    # Increase the number of epochs as the model hadn't finished learning.\n",
        "    num_train_epochs=6,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"wandb\",\n",
        ")"
      ],
      "metadata": {
        "id": "IYlIhZjh0u-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "tCZ80Y5-0w0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStarting LoRA fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "id": "kpWlBsh100k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def extract_metrics_from_trainer_robust(trainer):\n",
        "    \"\"\"\n",
        "    A more robust function to extract training and evaluation metrics from the\n",
        "    trainer's log history, handling different logging structures.\n",
        "    \"\"\"\n",
        "    history = trainer.state.log_history\n",
        "    epoch_data = []\n",
        "    current_training_loss = None\n",
        "\n",
        "    for log in history:\n",
        "        # A training log contains 'loss' but not 'eval_loss'\n",
        "        if 'loss' in log and 'eval_loss' not in log:\n",
        "            current_training_loss = log['loss']\n",
        "\n",
        "        # An evaluation log marks the end of an epoch's metrics\n",
        "        elif 'eval_loss' in log:\n",
        "            epoch_metrics = {\n",
        "                'Epoch': int(log['epoch']),\n",
        "                'Training Loss': current_training_loss,\n",
        "                'Validation Loss': log['eval_loss'],\n",
        "                'Accuracy': log['eval_accuracy'],\n",
        "                'F1 Score': log['eval_f1']\n",
        "            }\n",
        "            epoch_data.append(epoch_metrics)\n",
        "\n",
        "    # Create the final DataFrame\n",
        "    df = pd.DataFrame(epoch_data)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "nsQVVmyn15P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Extract metrics using the new, robust function\n",
        "# Make sure your trainer object is named 'trainer'.\n",
        "metrics_df = extract_metrics_from_trainer_robust(trainer)\n",
        "\n",
        "print(\"Extracted Metrics DataFrame:\")\n",
        "print(metrics_df)\n",
        "\n",
        "# Step 2: Use the extracted DataFrame to generate the plots\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
        "fig.suptitle('LoRA Fine-Tuning Metrics per Epoch (from Trainer History)', fontsize=16)\n",
        "\n",
        "# Plot 1: Training and Validation Loss\n",
        "ax1.plot(metrics_df['Epoch'], metrics_df['Training Loss'], 'o-', label='Training Loss')\n",
        "ax1.plot(metrics_df['Epoch'], metrics_df['Validation Loss'], 'o-', label='Validation Loss')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Model Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Plot 2: Accuracy and F1 Score\n",
        "ax2.plot(metrics_df['Epoch'], metrics_df['Accuracy'], 'o-', label='Validation Accuracy', color='g')\n",
        "ax2.plot(metrics_df['Epoch'], metrics_df['F1 Score'], 'o-', label='Validation F1 Score', color='r')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Score')\n",
        "ax2.set_title('Model Performance Metrics')\n",
        "ax2.legend()\n",
        "ax2.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Display the plot\n",
        "plt.xticks(metrics_df['Epoch'])\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UFhigZ7tCxlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K_enM3UBCznw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}