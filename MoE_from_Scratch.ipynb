{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install pandas\n",
        "!pip install mlflow"
      ],
      "metadata": {
        "id": "xb1PTDt0Vr1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import init\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os"
      ],
      "metadata": {
        "id": "R4RDUpNXVryp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add these imports at the top\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "CKWh699VeEwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "7mMrp0pYVrwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load your Banking77 dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "1k5z1Ve4Vrtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/Banking77_Project/data/train.csv')\n",
        "print(f\"Training data shape: {train_df.shape}\")\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "id": "cIsMrvJrVrrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text data\n",
        "text = \" \".join(train_df['text'].astype(str).tolist())\n",
        "print(f\"Total characters in dataset: {len(text)}\")\n",
        "print(text[:1000])  # Show first 1000 characters"
      ],
      "metadata": {
        "id": "AuUEeRPAVrok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create character-level vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(f\"Vocabulary size: {vocab_size}\")"
      ],
      "metadata": {
        "id": "UDWPcK6IWK_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "pHT651pfWK9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test encoding/decoding\n",
        "test_str = \"banking query\"\n",
        "encoded = encode(test_str)\n",
        "decoded = decode(encoded)\n",
        "print(f\"Original: {test_str}\")\n",
        "print(f\"Encoded: {encoded}\")\n",
        "print(f\"Decoded: {decoded}\")\n"
      ],
      "metadata": {
        "id": "7dn3HhTnWK67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this after your encode/decode functions\n",
        "def text_to_tokens(text, block_size):\n",
        "    encoded = encode(text)\n",
        "    if len(encoded) > block_size:\n",
        "        encoded = encoded[:block_size]\n",
        "    else:\n",
        "        encoded = encoded + [0] * (block_size - len(encoded))\n",
        "    return torch.tensor(encoded, dtype=torch.long).unsqueeze(0)\n"
      ],
      "metadata": {
        "id": "CasqosothbMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the entire text dataset\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(f\"Data shape: {data.shape}\")\n",
        "\n",
        "# Split into training and validation sets\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(f\"Training data size: {len(train_data)}\")\n",
        "print(f\"Validation data size: {len(val_data)}\")"
      ],
      "metadata": {
        "id": "kDDe-q_QWK4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64  # Increased batch size\n",
        "block_size = 256  # Longer context\n",
        "max_iters = 10000  # More training iterations\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-4  # Lower learning rate for better stability\n",
        "eval_iters = 250\n",
        "head_size = 64  # Larger head size\n",
        "n_embed = 512  # Increased embedding dimension\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "dropout = 0.1  # Reduced dropout\n",
        "num_experts = 8\n",
        "top_k = 2"
      ],
      "metadata": {
        "id": "8lpF3gtrWUIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading function\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "r4QgfM4_WUF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss estimation function\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "EbmpWN02WUDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Improved model components with better initialization\n",
        "class ImprovedHead(nn.Module):\n",
        "    \"\"\"Improved self-attention head with better initialization\"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "\n",
        "        # Better initialization\n",
        "        init.kaiming_uniform_(self.key.weight)\n",
        "        init.kaiming_uniform_(self.query.weight)\n",
        "        init.kaiming_uniform_(self.value.weight)\n",
        "\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "Hix7C6U5WUBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multiple heads of self-attention in parallel\"\"\"\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class ImprovedExpert(nn.Module):\n",
        "    \"\"\"Improved MLP expert with better architecture\"\"\"\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.GELU(),  # Changed to GELU for better performance\n",
        "            nn.LayerNorm(4 * n_embed),  # Added layer norm\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        # Better initialization\n",
        "        for layer in self.net:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                init.kaiming_uniform_(layer.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "tsHZVV4dWT-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NoisyTopkRouter(nn.Module):\n",
        "    \"\"\"Noisy top-k gating network\"\"\"\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(NoisyTopkRouter, self).__init__()\n",
        "        self.top_k = top_k\n",
        "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
        "        self.noise_linear = nn.Linear(n_embed, num_experts)\n",
        "\n",
        "    def forward(self, mh_output):\n",
        "        logits = self.topkroute_linear(mh_output)\n",
        "        noise_logits = self.noise_linear(mh_output)\n",
        "        noise = torch.randn_like(logits) * F.softplus(noise_logits)\n",
        "        noisy_logits = logits + noise\n",
        "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
        "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
        "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "        router_output = F.softmax(sparse_logits, dim=-1)\n",
        "        return router_output, indices"
      ],
      "metadata": {
        "id": "aQFgTTgHWuCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedSparseMoE(nn.Module):\n",
        "    \"\"\"Improved Sparse Mixture of Experts module\"\"\"\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(ImprovedSparseMoE, self).__init__()\n",
        "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
        "        self.experts = nn.ModuleList([ImprovedExpert(n_embed) for _ in range(num_experts)])\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def forward(self, x):\n",
        "        gating_output, indices = self.router(x)\n",
        "        final_output = torch.zeros_like(x)\n",
        "        flat_x = x.view(-1, x.size(-1))\n",
        "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
        "\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            expert_mask = (indices == i).any(dim=-1)\n",
        "            flat_mask = expert_mask.view(-1)\n",
        "\n",
        "            if flat_mask.any():\n",
        "                expert_input = flat_x[flat_mask]\n",
        "                expert_output = expert(expert_input)\n",
        "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
        "                weighted_output = expert_output * gating_scores\n",
        "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
        "\n",
        "        return final_output"
      ],
      "metadata": {
        "id": "J7w8jAQsWt_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedBlock(nn.Module):\n",
        "    \"\"\"Improved Transformer block\"\"\"\n",
        "    def __init__(self, n_embed, n_head, num_experts, top_k):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.smoe = ImprovedSparseMoE(n_embed, num_experts, top_k)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Residual connections with better scaling\n",
        "        x = x + self.dropout(self.sa(self.ln1(x)))\n",
        "        x = x + self.dropout(self.smoe(self.ln2(x)))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "C98Jj0kzWt84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedSparseMoELanguageModel(nn.Module):\n",
        "    \"\"\"Improved Sparse MoE Language Model\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[ImprovedBlock(n_embed, n_head, num_experts, top_k) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "        # Better initialization\n",
        "        init.kaiming_uniform_(self.token_embedding_table.weight)\n",
        "        init.kaiming_uniform_(self.position_embedding_table.weight)\n",
        "        init.kaiming_uniform_(self.lm_head.weight)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "NVBnZVzzWt51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize improved model\n",
        "model = ImprovedSparseMoELanguageModel()\n",
        "model = model.to(device)\n",
        "\n",
        "# Print number of parameters\n",
        "print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f} M parameters\")\n",
        "\n",
        "# Create optimizer with weight decay\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "\n",
        "# Corrected scheduler without the 'verbose' argument\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=200)"
      ],
      "metadata": {
        "id": "3PrS95wWh_Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "def kaiming_init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "model = SparseMoELanguageModel()\n",
        "model.apply(kaiming_init_weights)\n",
        "model = model.to(device)\n",
        "\n",
        "# Print number of parameters\n",
        "print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f} M parameters\")\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_iters, eta_min=learning_rate/10)"
      ],
      "metadata": {
        "id": "1hJUvstqWt2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n"
      ],
      "metadata": {
        "id": "oNbuboJcWtyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a sample from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_tokens = model.generate(context, max_new_tokens=2000)[0].tolist()\n",
        "\n",
        "# Decode the generated tokens back into text and print\n",
        "generated_text = decode(generated_tokens)\n",
        "print(generated_text)\n",
        "print(\"Generated text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "Sa07bQQDW9HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Banking77_Project/moe_model.pth')\n",
        "print(\"Model saved!\")"
      ],
      "metadata": {
        "id": "DpIkBV7HW89Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Testing Sample Predictions...\")\n",
        "\n",
        "model.eval()\n",
        "sample_texts = [\n",
        "    \"what is my account balance\",\n",
        "    \"transfer money to my friend\",\n",
        "    \"my card is not working\",\n",
        "    \"I want to apply for a loan\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "zFGM9GV7XGIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for text in sample_texts:\n",
        "        # Tokenize\n",
        "        token_ids = tokenizer.tokenize(text).unsqueeze(0).to(device)\n",
        "        attention_mask = (token_ids != 0).float()\n",
        "\n",
        "        # Predict\n",
        "        outputs = model(token_ids, attention_mask)\n",
        "        predicted_id = torch.argmax(outputs['logits'], dim=1).item()\n",
        "        confidence = torch.softmax(outputs['logits'], dim=1).max().item()\n",
        "\n",
        "        # Get expert usage\n",
        "        expert_weights = outputs['routing_weights'][0].cpu().numpy()\n",
        "        top_experts = [f\"E{i+1}({w:.2f})\" for i, w in enumerate(expert_weights) if w > 0.01]\n",
        "\n",
        "        print(f\"Text: '{text}'\")\n",
        "        print(f\"  → Predicted: {id2label[predicted_id]} (confidence: {confidence:.3f})\")\n",
        "        print(f\"  → Experts used: {', '.join(top_experts)}\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "BOruvqjoXUmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qAEmEMwfXWzh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}